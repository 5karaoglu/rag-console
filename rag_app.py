import pandas as pd
import json
import chromadb
from chromadb.utils import embedding_functions
import typer
from rich.console import Console
from rich.prompt import Prompt
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from pathlib import Path
import warnings
import traceback
import os
import glob
import sys
import time
import threading
from typing import List, Dict, Any
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import gc

# CUDA bellek ayarlarÄ±
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,garbage_collection_threshold:0.8"

console = Console()
app = typer.Typer()

class RAGSystem:
    def __init__(self, json_path: str):
        try:
            self.json_path = json_path
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            console.print(f"\nCihaz: {self.device}", style="yellow")
            
            # GPU sayÄ±sÄ±nÄ± ve Ã¶zelliklerini kontrol et
            if torch.cuda.is_available():
                self.gpu_count = torch.cuda.device_count()
                console.print(f"KullanÄ±labilir GPU sayÄ±sÄ±: {self.gpu_count}", style="yellow")
                for i in range(self.gpu_count):
                    gpu_name = torch.cuda.get_device_name(i)
                    console.print(f"GPU {i}: {gpu_name}", style="yellow")
            else:
                self.gpu_count = 0
                console.print("GPU bulunamadÄ±, CPU kullanÄ±lacak", style="red")
            
            # Hugging Face token kontrolÃ¼
            self.hf_token = os.getenv('HUGGING_FACE_TOKEN')
            if not self.hf_token:
                raise ValueError("HUGGING_FACE_TOKEN bulunamadÄ±! LÃ¼tfen .env dosyasÄ±na token'Ä±nÄ±zÄ± ekleyin.")
            
            self.setup_model()
            self.setup_database()
        except Exception as e:
            console.print("\nâŒ BaÅŸlatma HatasÄ±:", style="bold red")
            console.print(f"Hata MesajÄ±: {str(e)}", style="red")
            console.print("\nHata DetayÄ±:", style="bold red")
            console.print(traceback.format_exc(), style="red")
            raise e
        
    def setup_model(self):
        try:
            console.print("\nModel yÃ¼kleniyor...", style="yellow")
            console.print(f"Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", style="yellow")
            
            # CUDA Ã¶nbelleÄŸini temizle
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                console.print("CUDA Ã¶nbelleÄŸi temizlendi", style="green")
                
                # GPU bellek bilgilerini gÃ¶ster
                for i in range(torch.cuda.device_count()):
                    total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    free_mem = torch.cuda.memory_reserved(i) / (1024**3)
                    console.print(f"GPU {i}: Toplam Bellek: {total_mem:.2f} GiB, KullanÄ±labilir: {free_mem:.2f} GiB", style="yellow")
            
            self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            
            # Cache dizinleri iÃ§in bilgi
            cache_dir = os.getenv('TRANSFORMERS_CACHE', './cache/huggingface')
            console.print(f"Cache Konumu: {cache_dir}", style="yellow")
            
            # Tokenizer yapÄ±landÄ±rmasÄ±
            console.print("\nTokenizer yÃ¼kleniyor...", style="yellow")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token,
                trust_remote_code=True,
                padding_side="left",
                use_fast=True,  # HÄ±zlÄ± tokenizer kullan
                cache_dir=cache_dir,
                local_files_only=False
            )
            console.print("Tokenizer yÃ¼klendi!", style="green")
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Model yapÄ±landÄ±rmasÄ±
            console.print("\nModel dosyalarÄ± indiriliyor veya cache'den yÃ¼kleniyor...", style="yellow")
            
            # Cache durumunu kontrol et
            model_cache_path = os.path.join(cache_dir, "models--deepseek-ai--DeepSeek-R1-Distill-Qwen-32B")
            
            # Ã‡evre deÄŸiÅŸkeni kontrolÃ¼
            use_local_first = os.getenv('USE_LOCAL_MODEL_FIRST', 'false').lower() == 'true'
            
            # Daha detaylÄ± cache kontrolÃ¼
            model_files_complete = False
            if os.path.exists(model_cache_path):
                # Shard dosyalarÄ±nÄ± kontrol et
                shard_pattern = os.path.join(model_cache_path, "snapshots", "*", "model-*.safetensors")
                shard_files = glob.glob(shard_pattern)
                if len(shard_files) >= 8:  # DeepSeek modeli 8 shard iÃ§eriyor
                    model_files_complete = True
            
            if os.path.exists(model_cache_path) and use_local_first and model_files_complete:
                console.print(f"Model cache bulundu: {model_cache_path}", style="green")
                console.print("USE_LOCAL_MODEL_FIRST=true ayarlandÄ± ve tÃ¼m model dosyalarÄ± mevcut, Ã¶nce yerel model deneniyor", style="green")
                try_local_first = True
            else:
                if not os.path.exists(model_cache_path):
                    console.print("Model cache bulunamadÄ±, indiriliyor...", style="yellow")
                elif not model_files_complete:
                    console.print("Model cache bulundu fakat eksik dosyalar var, indiriliyor...", style="yellow")
                else:
                    console.print("Model cache bulundu fakat USE_LOCAL_MODEL_FIRST=false, indiriliyor...", style="yellow")
                try_local_first = False
            
            # GPU bellek yapÄ±landÄ±rmasÄ±
            max_memory = {}
            if self.gpu_count >= 2:
                # Ã‡oklu GPU iÃ§in bellek yapÄ±landÄ±rmasÄ±
                for i in range(self.gpu_count):
                    max_memory[i] = "15GiB"  # Her GPU iÃ§in bellek sÄ±nÄ±rÄ±
                max_memory["cpu"] = "30GB"  # CPU iÃ§in bellek sÄ±nÄ±rÄ±
            elif self.gpu_count == 1:
                # Tek GPU iÃ§in bellek yapÄ±landÄ±rmasÄ±
                max_memory[0] = "24GiB"
                max_memory["cpu"] = "30GB"
            else:
                # GPU yok, sadece CPU
                max_memory["cpu"] = "32GB"
            
            try:
                # Ã–nce yerel dosyalardan yÃ¼klemeyi dene
                if try_local_first:
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            token=self.hf_token,
                            torch_dtype=torch.bfloat16,
                            device_map="auto",  # Otomatik cihaz haritalamasÄ±
                            trust_remote_code=True,
                            low_cpu_mem_usage=True,
                            use_cache=True,
                            cache_dir=cache_dir,
                            local_files_only=True,
                            max_memory=max_memory,
                            offload_folder="offload",  # Gerekirse CPU'ya offload et
                            offload_state_dict=True,  # Durum sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ offload et
                        )
                        console.print("Model yerel cache'den yÃ¼klendi!", style="green")
                    except Exception as local_error:
                        console.print(f"Yerel cache'den yÃ¼kleme baÅŸarÄ±sÄ±z: {str(local_error)}", style="yellow")
                        raise local_error
                else:
                    raise FileNotFoundError("Yerel cache atlanÄ±yor")
                    
            except Exception as e:
                console.print("Online'dan model indiriliyor...", style="yellow")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    token=self.hf_token,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",  # Otomatik cihaz haritalamasÄ±
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_cache=True,
                    cache_dir=cache_dir,
                    local_files_only=False,
                    resume_download=True,
                    max_memory=max_memory,
                    offload_folder="offload",  # Gerekirse CPU'ya offload et
                    offload_state_dict=True,  # Durum sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ offload et
                )
                console.print("Model indirildi ve cache'e kaydedildi!", style="green")
            
            self.model.config.pad_token_id = self.tokenizer.pad_token_id
            
            # Model iÃ§in FP16 hassasiyetini etkinleÅŸtir
            if hasattr(self.model, "half") and torch.cuda.is_available():
                self.model = self.model.half()
                console.print("Model FP16 hassasiyetine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼", style="green")
            
            console.print("\nModel yapÄ±landÄ±rmasÄ± tamamlandÄ±!", style="green")
            
        except Exception as e:
            console.print("\nâŒ Model YÃ¼kleme HatasÄ±:", style="bold red")
            console.print(f"Hata MesajÄ±: {str(e)}", style="red")
            console.print("\nHata DetayÄ±:", style="bold red")
            console.print(traceback.format_exc(), style="red")
            raise e
        
    def setup_database(self):
        console.print("VeritabanÄ± hazÄ±rlanÄ±yor...", style="yellow")
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # Sentence transformer embedding fonksiyonunu kullan ve GPU'ya taÅŸÄ±
        model_kwargs = {'device': 'cuda'} if torch.cuda.is_available() else {}
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-mpnet-base-v2",
            model_kwargs=model_kwargs
        )
        
        # Koleksiyon oluÅŸtur veya var olanÄ± al
        self.collection = self.chroma_client.get_or_create_collection(
            name="json_data",
            embedding_function=self.embedding_function
        )
        
        # JSON verilerini yÃ¼kle ve veritabanÄ±na ekle
        if self.collection.count() == 0:
            self.load_json_data()
            
        console.print("VeritabanÄ± hazÄ±r!", style="green")
        
    def load_json_data(self):
        console.print("JSON verisi yÃ¼kleniyor...", style="yellow")
        try:
            with open(self.json_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            # JSON yapÄ±sÄ±nÄ± kontrol et
            if "Sheet1" in data:
                # Veriyi iÅŸle
                documents = []
                ids = []
                
                for idx, item in enumerate(data["Sheet1"]):
                    # SatÄ±rdaki tÃ¼m deÄŸerleri stringe Ã§evir ve birleÅŸtir
                    content = " ".join([f"{key}: {str(value)}" for key, value in item.items()])
                    documents.append(content)
                    ids.append(f"doc_{idx}")
                
                # Belgeleri batch halinde ekle (bellek kullanÄ±mÄ±nÄ± optimize etmek iÃ§in)
                batch_size = 100
                for i in range(0, len(documents), batch_size):
                    end_idx = min(i + batch_size, len(documents))
                    self.collection.add(
                        documents=documents[i:end_idx],
                        ids=ids[i:end_idx]
                    )
                    
                console.print(f"Toplam {len(documents)} belge eklendi.", style="green")
            else:
                console.print("JSON dosyasÄ± beklenen formatta deÄŸil.", style="red")
                raise ValueError("JSON dosyasÄ± 'Sheet1' anahtarÄ±nÄ± iÃ§ermiyor.")
                
        except Exception as e:
            console.print(f"JSON veri yÃ¼kleme hatasÄ±: {str(e)}", style="red")
            raise e
        console.print("JSON verisi yÃ¼klendi!", style="green")
        
    def query(self, question: str) -> str:
        console.print(f"\n[bold cyan]Soru:[/bold cyan] {question}")
        
        # Ä°ÅŸlem baÅŸlangÄ±Ã§ zamanÄ±nÄ± kaydet
        start_time = time.time()
        
        console.print("[bold yellow]Bellek temizleniyor...[/bold yellow]")
        # Bellek temizliÄŸi
        gc.collect()
        torch.cuda.empty_cache()
        
        console.print("[bold yellow]Benzer dÃ¶kÃ¼manlar aranÄ±yor...[/bold yellow]")
        # Benzer dÃ¶kÃ¼manlarÄ± bul
        query_start_time = time.time()
        results = self.collection.query(
            query_texts=[question],
            n_results=3
        )
        console.print(f"[green]Benzer dÃ¶kÃ¼manlar bulundu! ({time.time() - query_start_time:.2f} saniye)[/green]")
        
        console.print("[bold yellow]Prompt oluÅŸturuluyor...[/bold yellow]")
        # Prompt oluÅŸtur
        context = "\n".join(results["documents"][0])
        prompt = f"""### GÃ–REV:
AÅŸaÄŸÄ±da verilen baÄŸlam bilgilerini kullanarak kullanÄ±cÄ±nÄ±n sorusuna kapsamlÄ± ve doÄŸru bir yanÄ±t oluÅŸtur.

### BAÄLAM:
{context}

### SORU:
{question}

### TALÄ°MATLAR:
1. Ã–nce baÄŸlamÄ± dikkatlice analiz et ve soruyla ilgili Ã¶nemli bilgileri belirle.
2. AdÄ±m adÄ±m dÃ¼ÅŸÃ¼nerek yanÄ±tÄ± oluÅŸtur.
3. YanÄ±tÄ±n tamamen baÄŸlam iÃ§indeki bilgilere dayalÄ± olmalÄ±dÄ±r.
4. EÄŸer baÄŸlamda soruyu yanÄ±tlamak iÃ§in yeterli bilgi yoksa, "ÃœzgÃ¼nÃ¼m, bu soruyu yanÄ±tlamak iÃ§in yeterli bilgim yok" ÅŸeklinde belirt.
5. YanÄ±tÄ±nda baÄŸlamda olmayan bilgileri uydurma.
6. YanÄ±tÄ±nÄ± aÃ§Ä±k, anlaÅŸÄ±lÄ±r ve Ã¶z bir ÅŸekilde sunmaya Ã§alÄ±ÅŸ.

### YANITIM:
"""
        
        console.print("[bold yellow]Model yanÄ±tÄ± oluÅŸturuyor...[/bold yellow]")
        # Model ile yanÄ±t oluÅŸtur
        tokenize_start_time = time.time()
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True).to(self.device)
        console.print(f"[green]Tokenize iÅŸlemi tamamlandÄ±! ({time.time() - tokenize_start_time:.2f} saniye)[/green]")
        
        # GPU bellek durumunu gÃ¶ster
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3
                reserved_mem = torch.cuda.memory_reserved(i) / 1024**3
                allocated_mem = torch.cuda.memory_allocated(i) / 1024**3
                free_mem = total_mem - reserved_mem
                console.print(f"[blue]GPU {i} Bellek: Toplam={total_mem:.1f}GB, AyrÄ±lan={allocated_mem:.1f}GB, Rezerve={reserved_mem:.1f}GB, BoÅŸ={free_mem:.1f}GB[/blue]")
        
        # Bellek optimizasyonu iÃ§in batch size'Ä± kÃ¼Ã§Ã¼lt
        generate_start_time = time.time()
        console.print("[bold yellow]YanÄ±t oluÅŸturuluyor (bu iÅŸlem biraz zaman alabilir)...[/bold yellow]")
        
        # Ä°lerleme gÃ¶stergesi iÃ§in thread
        stop_progress = threading.Event()
        
        def show_progress():
            start = time.time()
            i = 0
            symbols = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]
            while not stop_progress.is_set():
                elapsed = time.time() - start
                i = (i + 1) % len(symbols)
                console.print(f"[cyan]{symbols[i]} Token Ã¼retiliyor... ({elapsed:.1f} saniye geÃ§ti)[/cyan]")
                time.sleep(1)
        
        # Ä°lerleme gÃ¶stergesini baÅŸlat
        progress_thread = threading.Thread(target=show_progress)
        progress_thread.daemon = True
        progress_thread.start()
        
        try:
            with torch.cuda.amp.autocast(dtype=torch.float16):  # FP16 hassasiyeti kullan
                console.print("[magenta]Model generate fonksiyonu Ã§aÄŸrÄ±lÄ±yor...[/magenta]")
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,  # YanÄ±t iÃ§in maksimum yeni token sayÄ±sÄ±nÄ± azalt
                    num_return_sequences=1,
                    temperature=0.3,  # Daha tutarlÄ± yanÄ±tlar iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
                    top_p=0.85,  # Nucleus sampling iÃ§in
                    do_sample=True,  # Ã‡eÅŸitlilik iÃ§in Ã¶rnekleme yap
                    no_repeat_ngram_size=3,  # TekrarlarÄ± Ã¶nle
                    repetition_penalty=1.2,  # TekrarlarÄ± cezalandÄ±r
                    pad_token_id=self.tokenizer.pad_token_id,
                    use_cache=True  # Ã–nbelleÄŸi kullan
                )
        finally:
            # Ä°lerleme gÃ¶stergesini durdur
            stop_progress.set()
            progress_thread.join(timeout=1)
        
        console.print(f"[green]YanÄ±t oluÅŸturuldu! ({time.time() - generate_start_time:.2f} saniye)[/green]")
        
        decode_start_time = time.time()
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        console.print(f"[green]Decode iÅŸlemi tamamlandÄ±! ({time.time() - decode_start_time:.2f} saniye)[/green]")
        
        # Toplam sÃ¼reyi gÃ¶ster
        total_time = time.time() - start_time
        console.print(f"[bold green]Toplam iÅŸlem sÃ¼resi: {total_time:.2f} saniye[/bold green]")
        
        return response.split("YANITIM:")[-1].strip()

@app.command()
def main(json_path: str = "Book1.json"):
    """
    RAG sistemini baÅŸlat ve kullanÄ±cÄ± sorularÄ±nÄ± yanÄ±tla.
    """
    try:
        rag = RAGSystem(json_path)
        console.print("\nğŸ¤– RAG Sistemi hazÄ±r! Ã‡Ä±kmak iÃ§in 'exit' yazÄ±n.\n", style="bold green")
        
        while True:
            try:
                question = Prompt.ask("\nSorunuzu yazÄ±n")
                
                if question.lower() == "exit":
                    break
                    
                with console.status("YanÄ±t oluÅŸturuluyor..."):
                    response = rag.query(question)
                    
                console.print("\nYanÄ±t:", style="bold blue")
                console.print(response, style="green")
            except EOFError:
                console.print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±lÄ±yor (EOF alÄ±ndÄ±)...", style="bold yellow")
                break
            except KeyboardInterrupt:
                console.print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±lÄ±yor (Ctrl+C)...", style="bold yellow")
                break
            
    except Exception as e:
        console.print(f"\nâŒ Hata: {str(e)}", style="bold red")

if __name__ == "__main__":
    app() 