import pandas as pd
import json
import chromadb
from chromadb.utils import embedding_functions
import typer
from rich.console import Console
from rich.prompt import Prompt
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from pathlib import Path
import warnings
import traceback
import os
import glob
import sys
from typing import List, Dict, Any
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import gc

# CUDA bellek ayarlarÄ±
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

console = Console()
app = typer.Typer()

class RAGSystem:
    def __init__(self, json_path: str):
        try:
            self.json_path = json_path
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            console.print(f"\nCihaz: {self.device}", style="yellow")
            
            # Hugging Face token kontrolÃ¼
            self.hf_token = os.getenv('HUGGING_FACE_TOKEN')
            if not self.hf_token:
                raise ValueError("HUGGING_FACE_TOKEN bulunamadÄ±! LÃ¼tfen .env dosyasÄ±na token'Ä±nÄ±zÄ± ekleyin.")
            
            self.setup_model()
            self.setup_database()
        except Exception as e:
            console.print("\nâŒ BaÅŸlatma HatasÄ±:", style="bold red")
            console.print(f"Hata MesajÄ±: {str(e)}", style="red")
            console.print("\nHata DetayÄ±:", style="bold red")
            console.print(traceback.format_exc(), style="red")
            raise e
        
    def setup_model(self):
        try:
            console.print("\nModel yÃ¼kleniyor...", style="yellow")
            console.print(f"Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", style="yellow")
            
            self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            
            # Cache dizinleri iÃ§in bilgi
            cache_dir = os.getenv('TRANSFORMERS_CACHE', './cache/huggingface')
            console.print(f"Cache Konumu: {cache_dir}", style="yellow")
            
            # Tokenizer yapÄ±landÄ±rmasÄ±
            console.print("\nTokenizer yÃ¼kleniyor...", style="yellow")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token,
                trust_remote_code=True,
                padding_side="left",
                use_fast=False,
                cache_dir=cache_dir,
                local_files_only=False
            )
            console.print("Tokenizer yÃ¼klendi!", style="green")
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Model yapÄ±landÄ±rmasÄ±
            console.print("\nModel dosyalarÄ± indiriliyor veya cache'den yÃ¼kleniyor...", style="yellow")
            
            # Cache durumunu kontrol et
            model_cache_path = os.path.join(cache_dir, "models--deepseek-ai--DeepSeek-R1-Distill-Qwen-32B")
            
            # Ã‡evre deÄŸiÅŸkeni kontrolÃ¼
            use_local_first = os.getenv('USE_LOCAL_MODEL_FIRST', 'false').lower() == 'true'
            
            # Daha detaylÄ± cache kontrolÃ¼
            model_files_complete = False
            if os.path.exists(model_cache_path):
                # Shard dosyalarÄ±nÄ± kontrol et
                shard_pattern = os.path.join(model_cache_path, "snapshots", "*", "model-*.safetensors")
                shard_files = glob.glob(shard_pattern)
                if len(shard_files) >= 8:  # DeepSeek modeli 8 shard iÃ§eriyor
                    model_files_complete = True
            
            if os.path.exists(model_cache_path) and use_local_first and model_files_complete:
                console.print(f"Model cache bulundu: {model_cache_path}", style="green")
                console.print("USE_LOCAL_MODEL_FIRST=true ayarlandÄ± ve tÃ¼m model dosyalarÄ± mevcut, Ã¶nce yerel model deneniyor", style="green")
                try_local_first = True
            else:
                if not os.path.exists(model_cache_path):
                    console.print("Model cache bulunamadÄ±, indiriliyor...", style="yellow")
                elif not model_files_complete:
                    console.print("Model cache bulundu fakat eksik dosyalar var, indiriliyor...", style="yellow")
                else:
                    console.print("Model cache bulundu fakat USE_LOCAL_MODEL_FIRST=false, indiriliyor...", style="yellow")
                try_local_first = False
            
            try:
                # Ã–nce yerel dosyalardan yÃ¼klemeyi dene
                if try_local_first:
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            token=self.hf_token,
                            torch_dtype=torch.bfloat16,
                            device_map="auto",  # Otomatik cihaz haritalamasÄ±
                            trust_remote_code=True,
                            low_cpu_mem_usage=True,
                            use_cache=True,
                            cache_dir=cache_dir,
                            local_files_only=True,
                            max_memory={0: "18GiB", 1: "18GiB"},  # Her iki GPU iÃ§in bellek sÄ±nÄ±rlamasÄ±
                            offload_folder="offload",  # Gerekirse CPU'ya offload et
                        )
                        console.print("Model yerel cache'den yÃ¼klendi!", style="green")
                    except Exception as local_error:
                        console.print(f"Yerel cache'den yÃ¼kleme baÅŸarÄ±sÄ±z: {str(local_error)}", style="yellow")
                        raise local_error
                else:
                    raise FileNotFoundError("Yerel cache atlanÄ±yor")
                    
            except Exception as e:
                console.print("Online'dan model indiriliyor...", style="yellow")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    token=self.hf_token,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",  # Otomatik cihaz haritalamasÄ±
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    use_cache=True,
                    cache_dir=cache_dir,
                    local_files_only=False,
                    resume_download=True,
                    max_memory={0: "18GiB", 1: "18GiB"},  # Her iki GPU iÃ§in bellek sÄ±nÄ±rlamasÄ±
                    offload_folder="offload",  # Gerekirse CPU'ya offload et
                )
                console.print("Model indirildi ve cache'e kaydedildi!", style="green")
            
            self.model.config.pad_token_id = self.tokenizer.pad_token_id
            console.print("\nModel yapÄ±landÄ±rmasÄ± tamamlandÄ±!", style="green")
            
        except Exception as e:
            console.print("\nâŒ Model YÃ¼kleme HatasÄ±:", style="bold red")
            console.print(f"Hata MesajÄ±: {str(e)}", style="red")
            console.print("\nHata DetayÄ±:", style="bold red")
            console.print(traceback.format_exc(), style="red")
            raise e
        
    def setup_database(self):
        console.print("VeritabanÄ± hazÄ±rlanÄ±yor...", style="yellow")
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # Sentence transformer embedding fonksiyonunu kullan
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-mpnet-base-v2"
        )
        
        # Koleksiyon oluÅŸtur veya var olanÄ± al
        self.collection = self.chroma_client.get_or_create_collection(
            name="json_data",
            embedding_function=self.embedding_function
        )
        
        # JSON verilerini yÃ¼kle ve veritabanÄ±na ekle
        if self.collection.count() == 0:
            self.load_json_data()
            
        console.print("VeritabanÄ± hazÄ±r!", style="green")
        
    def load_json_data(self):
        console.print("JSON verisi yÃ¼kleniyor...", style="yellow")
        try:
            with open(self.json_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            # JSON yapÄ±sÄ±nÄ± kontrol et
            if "Sheet1" in data:
                # Veriyi iÅŸle
                documents = []
                ids = []
                
                for idx, item in enumerate(data["Sheet1"]):
                    # SatÄ±rdaki tÃ¼m deÄŸerleri stringe Ã§evir ve birleÅŸtir
                    content = " ".join([f"{key}: {str(value)}" for key, value in item.items()])
                    documents.append(content)
                    ids.append(f"doc_{idx}")
                
                # Belgeleri batch halinde ekle (bellek kullanÄ±mÄ±nÄ± optimize etmek iÃ§in)
                batch_size = 100
                for i in range(0, len(documents), batch_size):
                    end_idx = min(i + batch_size, len(documents))
                    self.collection.add(
                        documents=documents[i:end_idx],
                        ids=ids[i:end_idx]
                    )
                    
                console.print(f"Toplam {len(documents)} belge eklendi.", style="green")
            else:
                console.print("JSON dosyasÄ± beklenen formatta deÄŸil.", style="red")
                raise ValueError("JSON dosyasÄ± 'Sheet1' anahtarÄ±nÄ± iÃ§ermiyor.")
                
        except Exception as e:
            console.print(f"JSON veri yÃ¼kleme hatasÄ±: {str(e)}", style="red")
            raise e
        console.print("JSON verisi yÃ¼klendi!", style="green")
        
    def query(self, question: str) -> str:
        # Bellek temizliÄŸi
        gc.collect()
        torch.cuda.empty_cache()
        
        # Benzer dÃ¶kÃ¼manlarÄ± bul
        results = self.collection.query(
            query_texts=[question],
            n_results=3
        )
        
        # Prompt oluÅŸtur
        context = "\n".join(results["documents"][0])
        prompt = f"""### GÃ–REV:
AÅŸaÄŸÄ±da verilen baÄŸlam bilgilerini kullanarak kullanÄ±cÄ±nÄ±n sorusuna kapsamlÄ± ve doÄŸru bir yanÄ±t oluÅŸtur.

### BAÄLAM:
{context}

### SORU:
{question}

### TALÄ°MATLAR:
1. Ã–nce baÄŸlamÄ± dikkatlice analiz et ve soruyla ilgili Ã¶nemli bilgileri belirle.
2. AdÄ±m adÄ±m dÃ¼ÅŸÃ¼nerek yanÄ±tÄ± oluÅŸtur.
3. YanÄ±tÄ±n tamamen baÄŸlam iÃ§indeki bilgilere dayalÄ± olmalÄ±dÄ±r.
4. EÄŸer baÄŸlamda soruyu yanÄ±tlamak iÃ§in yeterli bilgi yoksa, "ÃœzgÃ¼nÃ¼m, bu soruyu yanÄ±tlamak iÃ§in yeterli bilgim yok" ÅŸeklinde belirt.
5. YanÄ±tÄ±nda baÄŸlamda olmayan bilgileri uydurma.
6. YanÄ±tÄ±nÄ± aÃ§Ä±k, anlaÅŸÄ±lÄ±r ve Ã¶z bir ÅŸekilde sunmaya Ã§alÄ±ÅŸ.

### YANITIM:
"""
        
        # Model ile yanÄ±t oluÅŸtur
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # Bellek optimizasyonu iÃ§in batch size'Ä± kÃ¼Ã§Ã¼lt
        with torch.cuda.amp.autocast():  # Otomatik karÄ±ÅŸÄ±k hassasiyet kullan
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,  # YanÄ±t iÃ§in maksimum yeni token sayÄ±sÄ±
                num_return_sequences=1,
                temperature=0.3,  # Daha tutarlÄ± yanÄ±tlar iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
                top_p=0.85,  # Nucleus sampling iÃ§in
                do_sample=True,  # Ã‡eÅŸitlilik iÃ§in Ã¶rnekleme yap
                no_repeat_ngram_size=3,  # TekrarlarÄ± Ã¶nle
                repetition_penalty=1.2,  # TekrarlarÄ± cezalandÄ±r
                pad_token_id=self.tokenizer.pad_token_id
            )
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return response.split("YanÄ±t:")[-1].strip()

@app.command()
def main(json_path: str = "Book1.json"):
    """
    RAG sistemini baÅŸlat ve kullanÄ±cÄ± sorularÄ±nÄ± yanÄ±tla.
    """
    try:
        rag = RAGSystem(json_path)
        console.print("\nğŸ¤– RAG Sistemi hazÄ±r! Ã‡Ä±kmak iÃ§in 'exit' yazÄ±n.\n", style="bold green")
        
        while True:
            try:
                question = Prompt.ask("\nSorunuzu yazÄ±n")
                
                if question.lower() == "exit":
                    break
                    
                with console.status("YanÄ±t oluÅŸturuluyor..."):
                    response = rag.query(question)
                    
                console.print("\nYanÄ±t:", style="bold blue")
                console.print(response, style="green")
            except EOFError:
                console.print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±lÄ±yor (EOF alÄ±ndÄ±)...", style="bold yellow")
                break
            except KeyboardInterrupt:
                console.print("\n\nğŸ‘‹ Program sonlandÄ±rÄ±lÄ±yor (Ctrl+C)...", style="bold yellow")
                break
            
    except Exception as e:
        console.print(f"\nâŒ Hata: {str(e)}", style="bold red")

if __name__ == "__main__":
    app() 